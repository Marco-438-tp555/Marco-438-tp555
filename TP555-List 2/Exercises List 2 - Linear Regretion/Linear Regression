1 - RESP: Para um conjunto de treinamento com milhares de features, o algorítmo do Gradiente Descendente é utilizado porque além de ser 
          um algorítimo de fácil implementação. Com a utilização do erro médio quadrático, a superfície de erro resulta em uma superfície
          convexa, com um único mínimo global. Não há a necessidade da preocupação com matrizes mal condicionadas com determinante próximo 
          de zero. 
          Dentro das Técnicas do Gradiente Descendente, especificamente para este caso, utiliza-se o algorítmo do Gradiente Descendente 
          Estocástico que tem a convergência mais rápida porém não garantida (oscila em torno do mínimo). Modificações no passo de aprendizagem
          podem garantir a convergência.

2 - RESP: Em um conjunto de treinamento que possua features com escalas muito diferentes, a técnica do Gradiente Descendente Estocático que é
          uma técnica que é mais sensível a variações do passo de aprendizagem. O SGD tem um caminho direto para o mínimo, porém ele muda várias 
          vezes ao longo deste caminho. Sendo assim, quando um conjunto de treinamento tem variações de valores bem diferentes, os contornos da 
          superfície de erro terão formato elíptico, o que dificulta mais ainda a convergência.
          Uma solução para mitigação deste problema é a realização do processo de Escalonamento de Features, que consiste na normalização das 
          features em um intervalo fixo. Este processo é feito durante o pré processamento dos dados que ajuda a acelerar a convergência do gradiente
          descendente pois deixa as curvas de nível da superfície de erro circular.

3 - RESP: Se o algorítimo implementado é o Gradiente Descendente Batch, então a cada iteração todos os exemplos de treinamento do modelo são considerados.
          Neste caso, se o erro medido a cada época está aumentando constantemente então possivelmente o passo de aprendizagem deste algorítimo está muito
          alto. O que pode ser feito para minimizar o erro é diminuir o passo de aprendizagem até que o algorítmo encontre o caminho ótimo para um mínimo 
          global onde a convergência seja garantida.
          
4 - RESP: O algorítimo do Gradiente Descendente Estocástico é o que chega mais rápido a vizinhança da solução ótima, ou seja ele converge na média.
          O algorítimo do Gradiente Descendente Batch é o que realmente converge, desde que o passo de aprendizagem nao seja muito grande, sendo assim a 
          convergência será mais lenta porém garantida.
          Para garantir que os algorítimos que tem uma convergência mais rápida para uma média de valores realmente tenham uma convergência mais próxima 
          do mínimo global, são utilizados esquemas de variação do passo de aprendizagem como por exemplo o Decaimento por etapas ou degraus, o Decaimento
          Exponencial e o Decaimento Temporal.
          
5 - RESP: 
          a), b) e c). Código do Algorítimo Mini-Batch está comitado na pasta "Exercises List 2 / Linear Regression Codes / Exercise 5".
          d). O valor ótimo para o passo de aprendizagem neste algorítmo foi 0.4.
          e). Após o ajuste do passo de aprendizagem para minimizar o erro em torno do mínimo global, conclui-se que o algorítmo Mini-Batch diverge muito pouco
              até atingir o mínimo glogal convergindo na média mas muito próximo do ponto ótimo.
              No caso do Batch, este converge direto para o ponto ótimo rapidamente. 
              No caso do Estocástico, após o ajuste do passo de aprendizagem para minimizar o erro em torno do mínimo glogal, este converge mais lentamente com um
              passo de aprendizagem muito pequeno se comparado aos anteriores mas mesmo assim ele tem convergência para a média do ponto ótimo e maior vairância que o
              Mini-Batch em torno do mínimo global.
          f). Tanto no caso do algorítmo dd Batch quanto no Mini-Batch, com um passo de aprendizagem bem próximo tem-se uma convergência mais rápida para um ponto ótimo
              se comparado com o Estocástico que com o custo de se minimizar o erro tem-se uma convergencia mais demorada para um ponto ótimo.
              
6 - RESP:
          a) e b). Código do Algorítimo Mini-Batch está comitado na pasta "Exercises List 2 / Linear Regression Codes / Exercise 6".
          NOTA: Analisando o comportamento do algorítmo, sua convergência se da com passo de aprendizagem menor do que 0.007 e o ponto ótimo para o passo de 
                aprendizagem se da para o valor de 0.005.

7 - RESP:
          Os algorítmos do gradiente descendente em Batelada para as funções hipótese das letras a), b), c) e d) estão comitados na pasta 
          "Exercises List 2 / Linear Regression Codes / Exercise 7".
          
          A. Analisando os gráficos do Erro vs. Iterações vs. Erro para os algorítimos do Exercício 7, pode-se chegar a conclusão que tanto para a implementação das 
             funções hipótese com polinômios de segundo, terceiro e quarto graus, o erro no final do treinamento é o mesmo. Isso pode ser visualizado aumentando-se o
             número de iterações. Com isso, um polinômio de segundo grau já seria a melhor opção dado a convergência mais rápida com um maior passo de aprendizagem.
          
          B. Com os parâmetros das funções hipótese otimizados e analisando novamente os gráficos do Erro vs. Iterações, novamente a função hipótese que utiliza o 
             polinômio de segundo grau apresenta convergência que atende ao critério esperado, apenas um ajuste no passo de aprendizagem foi realizado para corrigir 
             o pequeno aumento no número de iterações para convergência no ponto ótimo.

8 - RESP:
          Os algorítmos do gradiente descendente em Batelada para as funções de escalonamento de features das letras a), b) e c) estão comitados na pasta 
          "Exercises List 2 / Linear Regression Codes / Exercise 8".
          
          d). O escalonamento de features é um processo que tem por objetivo equalizar as features, ou seja, ajustar as superfícies de contorno de modo que a covergência para
              o ponto ótimo seja mais suave e as features tenham o mesmo peso neste processo.
              Devido a natureza dos dados, em ambos os processos de escalnomento (algorítimos Min-Max e Padronização) utilizam de função Gaussiana tem se uma convergência 
              mais rápida para o ponto ótimo com relação ao algorítmo sem nenhum processo de escalonamento. 
              No escalonamento por padronização, a convergência para o ponto ótimo acontece com um número menor de iterações do que no processo de escalonamento Min-Max e pode 
              ser facilmente entendido pois a superfície de contorno são mais circulares e com isso as features tem quase o mesmo peso no processo de convergência.
          

