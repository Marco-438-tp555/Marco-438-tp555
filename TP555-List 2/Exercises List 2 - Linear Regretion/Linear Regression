1 - RESP: Para um conjunto de treinamento com milhares de features, o algorítmo do Gradiente Descendente é utilizado porque além de ser 
          um algorítimo de fácil implementação. Com a utilização do erro médio quadrático, a superfície de erro resulta em uma superfície
          convexa, com um único mínimo global. Não há a necessidade da preocupação com matrizes mal condicionadas com determinante próximo 
          de zero. 
          Dentro das Técnicas do Gradiente Descendente, especificamente para este caso, utiliza-se o algorítmo do Gradiente Descendente 
          Estocástico que tem a convergência mais rápida porém não garantida (oscila em torno do mínimo). Modificações no passo de aprendizagem
          podem garantir a convergência.

2 - RESP: Em um conjunto de treinamento que possua features com escalas muito diferentes, a técnica do Gradiente Descendente Estocático que é
          uma técnica que é mais sensível a variações do passo de aprendizagem. O SGD tem um caminho direto para o mínimo, porém ele muda várias 
          vezes ao longo deste caminho. Sendo assim, quando um conjunto de treinamento tem variações de valores bem diferentes, os contornos da 
          superfície de erro terão formato elíptico, o que dificulta mais ainda a convergência.
          Uma solução para mitigação deste problema é a realização do processo de Escalonamento de Features, que consiste na normalização das 
          features em um intervalo fixo. Este processo é feito durante o pré processamento dos dados que ajuda a acelerar a convergência do gradiente
          descendente pois deixa as curvas de nível da superfície de erro circular.

3 - RESP: Se o algorítimo implementado é o Gradiente Descendente Batch, então a cada iteração todos os exemplos de treinamento do modelo são considerados.
          Neste caso, se o erro medido a cada época está aumentando constantemente então possivelmente o passo de aprendizagem deste algorítimo está muito
          alto. O que pode ser feito para minimizar o erro é diminuir o passo de aprendizagem até que o algorítmo encontre o caminho ótimo para um mínimo 
          global onde a convergência seja garantida.
          
4 - RESP: O algorítimo do Gradiente Descendente Estocástico é o que chega mais rápido a vizinhança da solução ótima, ou seja ele converge na média.
          O algorítimo do Gradiente Descendente Batch é o que realmente converge, desde que o passo de aprendizagem nao seja muito grande, sendo assim a 
          convergência será mais lenta porém garantida.
          Para garantir que os algorítimos que tem uma convergência mais rápida para uma média de valores realmente tenham uma convergência mais próxima 
          do mínimo global, são utilizados esquemas de variação do passo de aprendizagem como por exemplo o Decaimento por etapas ou degraus, o Decaimento
          Exponencial e o Decaimento Temporal.
          
5 - RESP: 
          a), b) e c). Código do Algorítimo Mini-Batch está comitado na pasta "Exercises List 2 - Linear Regression Codes - Mini_Batch_gradient_descent_with_figures.ipynb".
          d). O valor ótimo para o passo de aprendizagem neste algorítmo foi 0.4.
          e). Após o ajuste do passo de aprendizagem para minimizar o erro em torno do mínimo global, conclui-se que o algorítmo Mini-Batch diverge muito pouco
              até atingir o mínimo glogal convergindo na média mas muito próximo do ponto ótimo.
              No caso do Batch, este converge direto para o ponto ótimo rapidamente. 
              No caso do Estocástico, após o ajuste do passo de aprendizagem para minimizar o erro em torno do mínimo glogal, este converge mais lentamente com um
              passo de aprendizagem muito pequeno se comparado aos anteriores mas mesmo assim ele tem convergência para a média do ponto ótimo e maior vairância que o
              Mini-Batch em torno do mínimo global.
          f). Tanto no caso do algorítmo dd Batch quanto no Mini-Batch, com um passo de aprendizagem bem próximo tem-se uma convergência mais rápida para um ponto ótimo
              se comparado com o Estocástico que com o custo de se minimizar o erro tem-se uma convergencia mais demorada para um ponto ótimo.
              
6 - RESP:
          a) e b). Código do Algorítimo Mini-Batch está comitado na pasta "Exercises List 2 - Linear Regression Codes - ...".
          
          
6 - RESP: Código está comitado na pasta "...".
          

