Exercise 1.
RESP: Exsiste a possibilidade de se obter melhores resultados combinando estes 5 modelos através da técnica do Conjunto de Treinamento
      (Ensemble Learning), onde se faz a utilização do algorítmo do Método dos Conjuntos (Ensemble Method). Esta técnica tem como base 
      o uso de um grupo de preditores que traz maior precisão nos resultados do que um preditor somente.
      Um exemplo desta técnica para criação de um preditor com maior desempenho, o classificador de votação (Voting Classifier), onde 
      se agrega as previsões de cada classificador para prever uma classe com maior número de votos.

Exerise 2.
RESP: Classificador de Votação Rígida: Este tem por base agregar as previsões de cada classificador e como resultado a classe a ser 
      prevista será a quer tiver o maior número de votos.
      Classificador de Votação Suave: Este realiza a votação baseando-se nas probabilidades individuais de cada classificador, partindo
      do pressuposto de que este possa fazer uso do método predict_proba(), e assim o Scikit-Learn possa prever a classe com a maior 
      probabilidade quando comparada a média das probabilidades de todos os classificadores de forma individual.
      
Exerise 3.
RESP: Ambos os preditores Bagging e Pasting podem ser treinados em paralelo, através de diferentes núcleos de CPU ou servidores. 
      Uma observação importante é que Bagging e Pasting permitem que as instâncias de treinamento sejam amostradas várias vezes 
      por vários preditores porém apenas Bagging permite que as instâncias serjam amostradas várias vezes pelo mesmo preditor.
     
Exerise 4.
RESP: O benefício da utilização da avaliação Out-of-Bag pode realizar uma estimativa de erro utilizando apenas o conjunto de treinamento 
      sem a necessidade de um conjunto e validação ou teste separadamente.

Exerise 5.
RESP: O que torna as árvores-extras mais aleatórias do que as florestas aleatórias comuns é a utilização de limites aleatórios para cada 
      feature ao invés de fazer uma busca por melhores limites possíveis. Com esta aleatoriedade, há uma relação de troca entre maior bias 
      e menor variância nos dados, isso torna Extra-Trees muito mais rápido de treinar do que as florestas aleatórias comuns, já que encontrar
      o melhor limite possível para cada feature em cada nó é uma das tarefas mais demoradas para o crescimento de uma árvore.



